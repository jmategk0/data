{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Sqooping Data Into Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqoop import --connect 'jdbc:sqlserver://bigdata220w18.database.windows.net:1433;database=week3' --driver 'com.microsoft.sqlserver.jdbc.SQLServerDriver' --connection-manager 'org.apache.sqoop.manager.SQLServerManager' --username 'bigdata' --password '5Zgv\\6;8rM' -target-dir '/data/rfm/test4' --split-by 'HH_DEMOGRAPHIC.HOUSEHOLD_KEY' --query 'SELECT HH_DEMOGRAPHIC.HOUSEHOLD_KEY, HH_DEMOGRAPHIC.HOMEOWNER_DESC, TRANSACTION_DATA.day, TRANSACTION_DATA.basket_id, TRANSACTION_DATA.sales_value, TRANSACTION_DATA.QUANTITY, PRODUCT.PRODUCT_ID FROM HH_DEMOGRAPHIC, TRANSACTION_DATA, PRODUCT WHERE HH_DEMOGRAPHIC.HOUSEHOLD_KEY = TRANSACTION_DATA.HOUSEHOLD_KEY AND TRANSACTION_DATA.PRODUCT_ID = PRODUCT.PRODUCT_ID AND $CONDITIONS' --as-parquetfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//import stuff for tests\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val ss = SparkSession.\n",
    "builder().\n",
    "master(\"local\").\n",
    "appName(\"Spark in Motion Example\").\n",
    "config(\"spark.config.option\", \"some-value\").\n",
    "enableHiveSupport().\n",
    "getOrCreate()\n",
    "\n",
    "import ss.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.util.Date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rfm_raw = spark.read.parquet(\"hdfs:///data/rfm/test4/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfm_raw.createOrReplaceTempView(\"rfm_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rfm_rpt = spark.sql(\"SELECT HOUSEHOLD_KEY, MIN(day) as recency, COUNT(DISTINCT basket_id) as frequency, SUM(sales_value) as monetary  FROM rfm_temp GROUP BY HOUSEHOLD_KEY ORDER BY recency, frequency, monetary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rfm_rpt_home = spark.sql(\"SELECT HOUSEHOLD_KEY, MIN(day) as recency, COUNT(DISTINCT basket_id) as frequency, SUM(sales_value) as monetary  FROM rfm_temp WHERE HOMEOWNER_DESC = 'Homeowner' GROUP BY HOUSEHOLD_KEY ORDER BY recency, frequency, monetary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfm_rpt.repartition(1).write.csv(\"hdfs:///data/rfm/results/rfm_all_report.csv\")\n",
    "rfm_rpt_home.repartition(1).write.csv(\"hdfs:///data/rfm/results/rfm_home_report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
