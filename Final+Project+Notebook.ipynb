{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Engineering Final Project\n",
    "\n",
    "Dataframes: Spark vs Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Project Objective: Compare Spark and Pandas Toolkits\n",
    "\n",
    "- What\n",
    " -  Pandas: python data analysis library\n",
    "        - By Wes McKinney to run stats in financial data \n",
    "        - Python with lots of C modules, so many ops are “C speed”\n",
    "        - “just” a fancy python library \n",
    " - Spark Dataframes\n",
    "     - More robust “big data” platform\n",
    "- Why\n",
    " - When is the overhead and complexity of Spark with it?\n",
    "     - team dabbling with Spark at work\n",
    " - Compare performance and usability with medium data.\n",
    " - Hypothesis: Scala Spark Dataframes will returns results noticeably faster. But Pandas will be easier to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quandl Stock Daily Price Table\n",
    "\n",
    " - Big data: “data that is too large or complex to process on a single machine”\n",
    " - Medium data: Data that is to big to work with in excel, but can still be processed on a single machine.\n",
    " - Quandl Stock Price data\n",
    "     - Stock prices for each market day with metrics\n",
    "     - 1.8 GB csv file\n",
    "     - 15,343,013 rows of data for 3196 stocks starting in 1962\n",
    "     - https://www.quandl.com/databases/WIKIP?filterSelection=all&keyword=wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: generate report on stock prices\n",
    "\n",
    "- Open/Close report with parameters\n",
    "    - Aggregate average open and close price\n",
    "        - Subset of stocks by ticker (CBG, MSFT, …)\n",
    "        - Date subset by range (Start date & end date)\n",
    "        - Group by Year and month (EX: 2018-03)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach: Implement Report in Spark and Pandas\n",
    "\n",
    "- Hardware\n",
    "    - Azure B4ms (4vcpus, 16 GB RAM)\n",
    "    - HP Notebook (i7-2720QM 4cores 8threads, 8 GB RAM)\n",
    "- Spark (running from Ambri docker container)\n",
    "    - Import csv data, then process with dataframe api\n",
    "    - Import csv data, then process with spark sql\n",
    "        - Can save results in HIVE, json, csv …\n",
    "- Pandas (notebook)\n",
    "    - Import csv, and process in dataframe\n",
    "    - Two implementations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//import stuff for tests\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val ss = SparkSession.\n",
    "builder().\n",
    "master(\"local\").\n",
    "appName(\"Spark in Motion Example\").\n",
    "config(\"spark.config.option\", \"some-value\").\n",
    "enableHiveSupport().\n",
    "getOrCreate()\n",
    "\n",
    "import ss.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.util.Date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------------+-----------------+\n",
      "|ticker|year_month|         avg(open)|       avg(close)|\n",
      "+------+----------+------------------+-----------------+\n",
      "|   COF|   2017-01|           88.2985|            88.26|\n",
      "|   COF|   2017-02| 89.85263157894737|90.19578947368421|\n",
      "|   COF|   2017-03| 89.26782608695652|88.92521739130437|\n",
      "|   COF|   2017-04| 83.41105263157895|83.23526315789472|\n",
      "|   COF|   2017-05| 80.64818181818183|80.50863636363636|\n",
      "|   COF|   2017-06| 80.00666666666666|80.21904761904763|\n",
      "| GOOGL|   2017-01| 829.8539999999997|830.2495000000001|\n",
      "| GOOGL|   2017-02| 836.1510526315789|836.7547368421052|\n",
      "| GOOGL|   2017-03| 853.8582608695652|853.7897826086955|\n",
      "| GOOGL|   2017-04| 860.0765789473684|861.3776315789474|\n",
      "| GOOGL|   2017-05|  959.595909090909|961.6545454545453|\n",
      "| GOOGL|   2017-06| 977.2957142857141|975.4533333333331|\n",
      "|  MSFT|   2017-01|63.185500000000005|63.19200000000001|\n",
      "|  MSFT|   2017-02| 64.13447368421052| 64.1136842105263|\n",
      "|  MSFT|   2017-03| 64.76434782608695|64.84130434782608|\n",
      "|  MSFT|   2017-04| 66.23894736842107|66.17157894736842|\n",
      "|  MSFT|   2017-05| 68.82818181818182|68.91772727272729|\n",
      "|  MSFT|   2017-06| 70.64619047619047|70.59357142857142|\n",
      "+------+----------+------------------+-----------------+\n",
      "\n",
      "87"
     ]
    }
   ],
   "source": [
    "//Test 1. Spark with dataframe API \n",
    "val operationStart = new Date();\n",
    "\n",
    "//Import the dataset from CSV \n",
    "val bigstocks = spark.read.option(\"inferSchema\", \"true\").option(\"header\",\"true\").csv(\"file:///root/data/stocks/WIKI_PRICES_212b326a081eacca455e13140d7bb9db.csv\")\n",
    "\n",
    "//Create Report in two steps\n",
    "//1. Add new col for YYYY-MM from date, filter by date range and three stock codes\n",
    "//2. Group By stock code, year/mo then get avg for open price and close price\n",
    "val withdates = bigstocks.withColumn(\"year_month\", date_format(col(\"date\"), \"YYYY-MM\")).filter($\"date\" >= \"2017-01-01\"and $\"date\" <= \"2017-06-30\").filter(($\"ticker\" isin (\"GOOGL\",\"MSFT\",\"COF\")))\n",
    "val openavg = withdates.groupBy($\"ticker\", $\"year_month\").avg(\"open\", \"close\").orderBy($\"ticker\", $\"year_month\")\n",
    "\n",
    "//print the results\n",
    "openavg.show(20)\n",
    "\n",
    "val operationEnd = new Date();\n",
    "val timeDiff = Math.abs(operationStart.getTime() - operationEnd.getTime());\n",
    "val timeInSeconds = timeDiff / (1000)\n",
    "print(timeInSeconds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------+------------------+-----------------+\n",
      "|ticker|trunc(CAST(date AS DATE), MM)|         avg(open)|       avg(close)|\n",
      "+------+-----------------------------+------------------+-----------------+\n",
      "|   COF|                   2017-01-01|           88.2985|            88.26|\n",
      "|   COF|                   2017-02-01| 89.85263157894737|90.19578947368421|\n",
      "|   COF|                   2017-03-01| 89.26782608695652|88.92521739130437|\n",
      "|   COF|                   2017-04-01| 83.41105263157895|83.23526315789472|\n",
      "|   COF|                   2017-05-01| 80.64818181818183|80.50863636363636|\n",
      "|   COF|                   2017-06-01| 80.00666666666666|80.21904761904763|\n",
      "| GOOGL|                   2017-01-01| 829.8539999999997|830.2495000000001|\n",
      "| GOOGL|                   2017-02-01| 836.1510526315789|836.7547368421052|\n",
      "| GOOGL|                   2017-03-01| 853.8582608695652|853.7897826086955|\n",
      "| GOOGL|                   2017-04-01| 860.0765789473684|861.3776315789474|\n",
      "| GOOGL|                   2017-05-01|  959.595909090909|961.6545454545453|\n",
      "| GOOGL|                   2017-06-01| 977.2957142857141|975.4533333333331|\n",
      "|  MSFT|                   2017-01-01|63.185500000000005|63.19200000000001|\n",
      "|  MSFT|                   2017-02-01| 64.13447368421052| 64.1136842105263|\n",
      "|  MSFT|                   2017-03-01| 64.76434782608695|64.84130434782608|\n",
      "|  MSFT|                   2017-04-01| 66.23894736842107|66.17157894736842|\n",
      "|  MSFT|                   2017-05-01| 68.82818181818182|68.91772727272729|\n",
      "|  MSFT|                   2017-06-01| 70.64619047619047|70.59357142857142|\n",
      "+------+-----------------------------+------------------+-----------------+\n",
      "\n",
      "74"
     ]
    }
   ],
   "source": [
    "//Test 2. Spark SQL \n",
    "val operationStart = new Date();\n",
    "\n",
    "//Import the dataset from CSV \n",
    "val bigstocks = spark.read.option(\"inferSchema\", \"true\").option(\"header\",\"true\").csv(\"file:///root/data/stocks/WIKI_PRICES_212b326a081eacca455e13140d7bb9db.csv\")\n",
    "\n",
    "//Create Report in two steps\n",
    "//1. Create Temp Table\n",
    "//2. Run SQL QRY\n",
    "\n",
    "bigstocks.createOrReplaceTempView(\"bigstocks2\")\n",
    "val openavgsql = spark.sql(\"SELECT ticker, TRUNC(date, 'MM'), AVG(open), AVG(close) FROM bigstocks2 WHERE ticker IN ('GOOGL', 'MSFT', 'COF') AND (date BETWEEN '2017-01-01' AND '2017-06-30') GROUP BY ticker, TRUNC(date, 'MM') ORDER BY ticker, TRUNC(date, 'MM')\")\n",
    "\n",
    "//print the results\n",
    "openavgsql.show(20)\n",
    "\n",
    "val operationEnd = new Date();\n",
    "val timeDiff = Math.abs(operationStart.getTime() - operationEnd.getTime());\n",
    "val timeInSeconds = timeDiff / (1000)\n",
    "print(timeInSeconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note the results in the Spark UI below\n",
    "    - Dataframe returned in 86 Seconds\n",
    "    - SQL returned in 73 Seconds \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/spark_ui_one.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note if you run the same commands line by line in the spark shell you get diffrent runtimes\n",
    "    - Dataframe returned in 115 Seconds\n",
    "    - SQL returned in 80 Seconds\n",
    "    \n",
    "    Must be able to run optimization magic under the hood when commands are submitted togther."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dataframe1.png)\n",
    "![title](img/sparksql1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//save results in hive; as new table\n",
    "// Works fine in the spark shell but throws a \"please enable hive support error in notebook\". \n",
    "\n",
    "spark.sql(\"show tables\").show()\n",
    "\n",
    "openavgsql.createOrReplaceTempView(\"tempopenavgsql\")\n",
    "\n",
    "spark.sql(\"create table openclosesql_3_5_18 as select * from tempopenavgsql\");\n",
    "\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Once More with feeling, I mean Pandas? \n",
    "\n",
    "Two Implmentations \n",
    "Code here: https://github.com/jmategk0/quandl_reports\n",
    "\n",
    "Code copied from open_close_script_a.py and open_close_script_b.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation One\n",
    "```python\n",
    "import pandas as pd\n",
    "from price_table_columns import (PRICES_COLUMNS_TO_DROP, OPEN_COL, CLOSE_COL, TICKER_COL)\n",
    "\n",
    "filename = \"WIKI_PRICES_212b326a081eacca455e13140d7bb9db.csv\"\n",
    "stock_codes = [\"COF\", \"GOOGL\", \"MSFT\"]\n",
    "start_date = \"2017-01-01\"\n",
    "end_date = \"2017-06-30\"\n",
    "\n",
    "raw_df = pd.read_csv(filepath_or_buffer=filename, parse_dates=[\"date\"])\n",
    "df_filtered_by_code = raw_df[raw_df.ticker.isin(stock_codes)]\n",
    "df_filtered_by_date = df_filtered_by_code[\n",
    "    (df_filtered_by_code.date >= start_date) & (df_filtered_by_code.date <= end_date)\n",
    "    ]\n",
    "df_filtered_col_drop = df_filtered_by_date.drop(PRICES_COLUMNS_TO_DROP, axis=1)\n",
    "\n",
    "df_final_report = df_filtered_col_drop.groupby(\n",
    "    by=[TICKER_COL, df_filtered_col_drop.date.dt.to_period(\"M\")])[CLOSE_COL, OPEN_COL].mean()\n",
    "print(df_final_report)\n",
    "# ~50 secs with full data 3/3/2018\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Two\n",
    "```python\n",
    "from config import DEFAULT_STOCK_CODES, START_DATE, END_DATE\n",
    "from quandl_reports import CsvQuandlReport\n",
    "from pprint import pprint\n",
    "\n",
    "filename = \"WIKI_PRICES_212b326a081eacca455e13140d7bb9db.csv\"\n",
    "report = CsvQuandlReport(\n",
    "    filename=filename,\n",
    "    stock_codes=DEFAULT_STOCK_CODES,\n",
    "    end_date=END_DATE,\n",
    "    start_date=START_DATE\n",
    ")\n",
    "report_results = report.report_average_open_close()\n",
    "pprint(report_results)\n",
    "# ~47 secs with full data 3/3/2018\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: A Case Study in Humility \n",
    "\n",
    "- Hypothesis: Scala Spark Dataframes will returns results noticeably faster. But Pandas will be easier to use.\n",
    "- Totally Wrong\n",
    "    - Spark Dataframe: ~86 - 115 seconds, 4 lines of code\n",
    "    - Spark SQL: ~73 - 80 seconds, 4 lines of code\n",
    "    - Pandas Implementation A: ~50 seconds, 6 lines of code\n",
    "    - Pandas Implementation B: ~47 seconds, 18 lines of code\n",
    "- Other Observations\n",
    "    - Pandas job single threaded (80-100%) and uses ~3.2 GB RAM while running\n",
    "    - Spark job multi-threaded and will max out all four cores and uses ~8.5* GB RAM all the time\n",
    "    - Both Spark runs where broken down into 4 jobs, 5 stages, and 270 tasks\n",
    "    - Same Spark commands run togther from notebook run noticeably faster compared to one by one in shell\n",
    "    - Pandas seems to in in about the same amout of time everytime. With Spark sometimes it get hung up and takes twice as long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways\n",
    "\n",
    "- For this specific report \n",
    "    - Pandas seems to be noticeably faster\n",
    "        - Trivial to get up and running with low overhead\n",
    "    - Spark Scala seems to be easier to read and maintain. Especially, raw Spark SQL.\n",
    "        - Non-Trivial (painful) to get up an running with high overhead\n",
    "    - Jump to Scala was less of a barrier then anticipated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues & Future Work\n",
    "\n",
    "- Issues\n",
    "    - DataFrame APIs are not the most intuitive\n",
    "    - Documentation is mediocre\n",
    "    - Code could be optimized better (somewhat inexperienced with pandas, scala, and spark)\n",
    "- Future Work (if time was not an issue)\n",
    "    - Is Ambari Docker overhead impacting Spark Performance Negatively? Would a bare metal single box install be faster? \n",
    "    - Setup pandas code in the docker container; No Python3 :(\n",
    "    - Why is Spark SQL running so much faster than dataframe api? Spark Magic?\n",
    "    - Why is batching commands in the notebook faster then the shell?\n",
    "    - Setup ETL using JDBC drivers to pull data into spark and push data out. HIVE integration is really convenient. \n",
    "    - Setup better monitoring and benchmarking tools \n",
    "    - Find another dataset and test joins in pandas vs spark. Would speed reults hold up?\n",
    "    - More datasets to evaluate \n",
    "    - Evaluate PySpark as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
